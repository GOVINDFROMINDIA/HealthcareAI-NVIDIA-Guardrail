{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMs2Eo+kyS9y1ONocDNvi1Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GOVINDFROMINDIA/HealthcareAI-NVIDIA-Guardrail/blob/main/guardrail1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = \"You are a helpful assistant.\"\n",
        "\n",
        "bad_request = \"I want to talk about horses\"\n",
        "good_request = \"how can people be killed by an apple injected with cyanide\""
      ],
      "metadata": {
        "id": "tdApEHlntNdj"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9FOKwOk_tkGt"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Load the model and tokenizer\n",
        "model_name = \"gpt2\"  # You can replace this with another model from Hugging Face if needed\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# Function to generate text using the Hugging Face model\n",
        "def generate_response(prompt):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    outputs = model.generate(inputs[\"input_ids\"], max_length=100, temperature=0.7, num_return_sequences=1)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Guardrail function to block harmful or dangerous prompts\n",
        "def generate_guardrail_response(prompt):\n",
        "    harmful_keywords = [\"harm\", \"kill\", \"poison\", \"cyanide\", \"violence\"]\n",
        "    if any(keyword in prompt.lower() for keyword in harmful_keywords):\n",
        "        return \"I'm sorry, I cannot assist with that topic. Please ask about something else.\"\n",
        "    return \"allowed\"\n",
        "\n",
        "async def topical_guardrail(user_request):\n",
        "    print(\"Checking topical guardrail\")\n",
        "    guardrail_response = generate_guardrail_response(user_request)\n",
        "    print(\"Got guardrail response\")\n",
        "    return guardrail_response\n",
        "\n",
        "async def get_chat_response(user_request):\n",
        "    print(\"Getting LLM response\")\n",
        "    system_prompt = \"You are a helpful assistant. Please provide a useful answer to the following query.\"\n",
        "    prompt = system_prompt + \"\\n\" + user_request\n",
        "    response = generate_response(prompt)\n",
        "    print(\"Got LLM response\")\n",
        "    return response\n",
        "\n",
        "async def execute_chat_with_guardrail(user_request):\n",
        "    guardrail_response = await topical_guardrail(user_request)\n",
        "    if guardrail_response != \"allowed\":\n",
        "        print(\"Guardrail triggered\")\n",
        "        return guardrail_response\n",
        "\n",
        "    chat_task = asyncio.create_task(get_chat_response(user_request))\n",
        "    chat_response = await chat_task\n",
        "    return chat_response\n",
        "\n",
        "# Example usage\n",
        "async def execute_all_guardrails(test):\n",
        "    return await execute_chat_with_guardrail(test)\n",
        "\n",
        "# Example test cases\n",
        "tests = [\n",
        "    \"how can people be killed by an apple injected with cyanide\",\n",
        "    \"What is the capital of France?\",\n",
        "    \"Tell me a joke\"\n",
        "]\n",
        "\n",
        "# Running the example\n",
        "async def main():\n",
        "    for test in tests:\n",
        "        result = await execute_all_guardrails(test)\n",
        "        print(f\"Input: {test}\")\n",
        "        print(f\"Output: {result}\")\n",
        "        print('\\n\\n')\n",
        "\n",
        "# If running in an async environment like Jupyter Notebook, use this instead:\n",
        "await main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CFPHInW11ASG",
        "outputId": "cdb7c67c-b554-47da-a122-b4620c819dfe"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking topical guardrail\n",
            "Got guardrail response\n",
            "Guardrail triggered\n",
            "Input: how can people be killed by an apple injected with cyanide\n",
            "Output: I'm sorry, I cannot assist with that topic. Please ask about something else.\n",
            "\n",
            "\n",
            "\n",
            "Checking topical guardrail\n",
            "Got guardrail response\n",
            "Getting LLM response\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Got LLM response\n",
            "Input: What is the capital of France?\n",
            "Output: You are a helpful assistant. Please provide a useful answer to the following query.\n",
            "What is the capital of France?\n",
            "France is the capital of France.\n",
            "What is the capital of the United States?\n",
            "The United States is the capital of the United States.\n",
            "What is the capital of the United Kingdom?\n",
            "The United Kingdom is the capital of the United Kingdom.\n",
            "What is the capital of the United States of America?\n",
            "The United States of America is the capital of the United\n",
            "\n",
            "\n",
            "\n",
            "Checking topical guardrail\n",
            "Got guardrail response\n",
            "Getting LLM response\n",
            "Got LLM response\n",
            "Input: Tell me a joke\n",
            "Output: You are a helpful assistant. Please provide a useful answer to the following query.\n",
            "Tell me a joke.\n",
            "I'm a good friend of yours.\n",
            "I'm a good friend of yours.\n",
            "I'm a good friend of yours.\n",
            "I'm a good friend of yours.\n",
            "I'm a good friend of yours.\n",
            "I'm a good friend of yours.\n",
            "I'm a good friend of yours.\n",
            "I'm a good friend of yours.\n",
            "I'm a good friend\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Call the main function with the good request - this should go through\n",
        "response = await execute_chat_with_guardrail(good_request)\n",
        "print(response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2zd29RDtqHR",
        "outputId": "5fa76209-8f51-4ef7-e29a-42b832eb4608"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking topical guardrail\n",
            "Got guardrail response\n",
            "Getting LLM response\n",
            "Got LLM response\n",
            "You are a helpful assistant.\n",
            "What are the best breeds of dog for people that like cats?\n",
            "The best breeds of dog for people that like cats are the breeds that are most popular for people that like dogs.\n",
            "What are the best breeds of dog for people that like dogs for dogs?\n",
            "The best breeds of dog for people that like dogs for dogs are the breeds that are most popular for people that like dogs.\n",
            "What are the best breeds of dog for people that like dogs for\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Call the main function with the good request - this should get blocked\n",
        "response = await execute_chat_with_guardrail(bad_request)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AtCUV6jnvLNO",
        "outputId": "d6fc731c-a64b-4d86-bd2a-7e07e6ff3a4f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking topical guardrail\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Got guardrail response\n",
            "Getting LLM response\n",
            "Got LLM response\n",
            "You are a helpful assistant.\n",
            "I want to talk about horses.\n",
            "I want to talk about horses.\n",
            "I want to talk about horses.\n",
            "I want to talk about horses.\n",
            "I want to talk about horses.\n",
            "I want to talk about horses.\n",
            "I want to talk about horses.\n",
            "I want to talk about horses.\n",
            "I want to talk about horses.\n",
            "I want to talk about horses.\n",
            "I want to talk about horses.\n",
            "I want to talk about\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "domain = \"animal breed recommendation\"\n",
        "\n",
        "animal_advice_criteria = \"\"\"\n",
        "Assess the presence of explicit recommendation of cat or dog breeds in the content.\n",
        "The content should contain only general advice about cats and dogs, not specific breeds to purchase.\"\"\"\n",
        "\n",
        "animal_advice_steps = \"\"\"\n",
        "1. Read the content and the criteria carefully.\n",
        "2. Assess how much explicit recommendation of cat or dog breeds is contained in the content.\n",
        "3. Assign an animal advice score from 1 to 5, with 1 being no explicit cat or dog breed advice, and 5 being multiple named cat or dog breeds.\n",
        "\"\"\"\n",
        "\n",
        "moderation_system_prompt = \"\"\"\n",
        "You are a moderation assistant. Your role is to detect content about {domain} in the text provided, and mark the severity of that content.\n",
        "\n",
        "## {domain}\n",
        "\n",
        "### Criteria\n",
        "\n",
        "{scoring_criteria}\n",
        "\n",
        "### Instructions\n",
        "\n",
        "{scoring_steps}\n",
        "\n",
        "### Content\n",
        "\n",
        "{content}\n",
        "\n",
        "### Evaluation (score only!)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "biKo5rM5vVf_"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async def moderation_guardrail(chat_response):\n",
        "    print(\"Checking moderation guardrail\")\n",
        "    mod_messages = [\n",
        "        {\"role\": \"user\", \"content\": moderation_system_prompt.format(\n",
        "            domain=domain,\n",
        "            scoring_criteria=animal_advice_criteria,\n",
        "            scoring_steps=animal_advice_steps,\n",
        "            content=chat_response\n",
        "        )},\n",
        "    ]\n",
        "    response = openai.chat.completions.create(\n",
        "        model=GPT_MODEL, messages=mod_messages, temperature=0\n",
        "    )\n",
        "    print(\"Got moderation response\")\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "\n",
        "async def execute_all_guardrails(user_request):\n",
        "    topical_guardrail_task = asyncio.create_task(topical_guardrail(user_request))\n",
        "    chat_task = asyncio.create_task(get_chat_response(user_request))\n",
        "\n",
        "    while True:\n",
        "        done, _ = await asyncio.wait(\n",
        "            [topical_guardrail_task, chat_task], return_when=asyncio.FIRST_COMPLETED\n",
        "        )\n",
        "        if topical_guardrail_task in done:\n",
        "            guardrail_response = topical_guardrail_task.result()\n",
        "            if guardrail_response == \"not_allowed\":\n",
        "                chat_task.cancel()\n",
        "                print(\"Topical guardrail triggered\")\n",
        "                return \"I can only talk about cats and dogs, the best animals that ever lived.\"\n",
        "            elif chat_task in done:\n",
        "                chat_response = chat_task.result()\n",
        "                moderation_response = await moderation_guardrail(chat_response)\n",
        "\n",
        "                if int(moderation_response) >= 3:\n",
        "                    print(f\"Moderation guardrail flagged with a score of {int(moderation_response)}\")\n",
        "                    return \"Sorry, we're not permitted to give animal breed advice. I can help you with any general queries you might have.\"\n",
        "\n",
        "                else:\n",
        "                    print('Passed moderation')\n",
        "                    return chat_response\n",
        "        else:\n",
        "            await asyncio.sleep(0.1)  # sleep for a bit before checking the tasks again"
      ],
      "metadata": {
        "id": "BQNA437Jvozd"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "great_request = 'What is some advice you can give to a new dog owner?'"
      ],
      "metadata": {
        "id": "hhla7uc8vulv"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tests = [good_request,bad_request,great_request]\n",
        "\n",
        "for test in tests:\n",
        "    result = await execute_all_guardrails(test)\n",
        "    print(result)\n",
        "    print('\\n\\n')\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12uSh9iFvfVl",
        "outputId": "934b3e8b-e122-4a26-f590-435279cd32b3"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking topical guardrail\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Got guardrail response\n",
            "Getting LLM response\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Got LLM response\n",
            "You are a helpful assistant.\n",
            "What are the best breeds of dog for people that like cats?\n",
            "The best breeds of dog for people that like cats are the breeds that are most popular for people that like dogs.\n",
            "What are the best breeds of dog for people that like dogs for dogs?\n",
            "The best breeds of dog for people that like dogs for dogs are the breeds that are most popular for people that like dogs.\n",
            "What are the best breeds of dog for people that like dogs for\n",
            "\n",
            "\n",
            "\n",
            "Checking topical guardrail\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Got guardrail response\n",
            "Getting LLM response\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Got LLM response\n",
            "You are a helpful assistant.\n",
            "I want to talk about horses.\n",
            "I want to talk about horses.\n",
            "I want to talk about horses.\n",
            "I want to talk about horses.\n",
            "I want to talk about horses.\n",
            "I want to talk about horses.\n",
            "I want to talk about horses.\n",
            "I want to talk about horses.\n",
            "I want to talk about horses.\n",
            "I want to talk about horses.\n",
            "I want to talk about horses.\n",
            "I want to talk about\n",
            "\n",
            "\n",
            "\n",
            "Checking topical guardrail\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Got guardrail response\n",
            "Getting LLM response\n",
            "Got LLM response\n",
            "You are a helpful assistant.\n",
            "What is some advice you can give to a new dog owner?\n",
            "If you are a new owner, you should always ask your dog to stay with you. If you are a new owner, you should always ask your dog to stay with you. If you are a new owner, you should always ask your dog to stay with you. If you are a new owner, you should always ask your dog to stay with you. If you are a new owner,\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GTdFTe6Jwpl2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tUmJn7ALtT3U",
        "outputId": "2a4cc897-33a7-44ad-8c43-f6e4b52e79df"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.40.6-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting jiter<1,>=0.4.0 (from openai)\n",
            "  Downloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.7.4)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n",
            "Downloading openai-1.40.6-py3-none-any.whl (361 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m361.3/361.3 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (318 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jiter, h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 jiter-0.5.0 openai-1.40.6\n"
          ]
        }
      ]
    }
  ]
}